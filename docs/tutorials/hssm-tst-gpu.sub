#!/usr/bin/env bash
#SBATCH -J hssm_gpu
#SBATCH -A bornstea_lab_gpu
#SBATCH -p gpu
#SBATCH --gres=gpu:V100:1
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=16G   # 4 * 16G = 64G
#SBATCH --time=5-00:00:00
#SBATCH --array=0-2
#SBATCH --mail-type=end
#SBATCH --mail-user=jungsuy@uci.edu
set -euo pipefail

mkdir -p logs
cd "${SLURM_SUBMIT_DIR:-$PWD}"

CHAIN_ID="${SLURM_ARRAY_TASK_ID}"
SSC="${1}"

# Map the current array task to a concrete Slurm job id (handles arrays)
if [[ -n "${SLURM_ARRAY_TASK_ID:-}" ]]; then
  JOBID_TGT="${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}"
else
  JOBID_TGT="${SLURM_JOB_ID}"
fi
scontrol update JobId="$JOBID_TGT" JobName="s${SSC}_c${CHAIN_ID}" || true
scontrol update JobId="$JOBID_TGT" Comment="ssc=${SSC} chain=${CHAIN_ID}" || true

# Use your conda env with pip-managed CUDA 12 JAX
PY="/usr/people/jy4657/.conda/envs/hssm/bin/python"

# Do NOT load CUDA modules when using pip CUDA wheels
unset XLA_FLAGS
export PYTHONUNBUFFERED=1
export PYTHONNOUSERSITE=1
export JAX_ENABLE_X64=0
export XLA_PYTHON_CLIENT_MEM_FRACTION=.90

nvidia-smi || true
"$PY" - <<'PY'
import jax, sys, os
print("which python:", sys.executable)
print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES"))
print("JAX devices:", jax.devices(), "backend:", jax.default_backend())
PY

# Only rank 0 should execute payload (extra safety)
if [[ "${SLURM_PROCID:-0}" != "0" ]]; then exit 0; fi

python3 -u hssm-tst-gpu.py \
  --ssc ${1} \
  --chain-id $SLURM_ARRAY_TASK_ID \
  --draws 5000 \
  --tune 5000 \
  --sampler nuts_numpyro \
  --outdir chains \
  --chains 1
# echo "Saved: ${OUTFILE}"
echo "Done @ $(date)"
